{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# SparkSession - a new entry point\n\nIn Spark 2.0, spark introduced SparkSession\n\n### Creating a SparkSession\n\nA SparkSession can be created using a builder pattern. The builder will automatically reuse an existing SparkContext if one exists; and create a SparkContext if it does not exist. Configuration options set in the builder are automatically propagated over to Spark and Hadoop during I/O. \n\nA SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nsparkSession = SparkSession.builder\\\n  .master(\"local\")\\\n  .appName(\"my-spark-app\")\\\n  .config(\"spark.some.config.option\", \"config-value\")\\\n  .getOrCreate()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["sparkSession"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["In Databricks notebooks and Spark REPL, the SparkSession has been created automatically and assigned to variable \"spark\"."],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Unified entry point for reading data\n\nSparkSession is the entry point for reading data, similar to the old SQLContext.read."],"metadata":{}},{"cell_type":"code","source":["diamonds = spark.read.csv(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(diamonds)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Dataframes\n\nThe primary abstraction in Spark\n  - Immutable once constructed\n  - Track lineage information to efficiently recompute lost data \n  - Enable operations on collection of elements in parallel\n  \n#### Creating Dataframes  \nDataFrames can be constructed:\n  - by parallelizing existing Python collections (lists) \n  - by transforming an existing Spark or pandas DFs\n  - from files in HDFS or any other storage system\n  \nEach row of a DataFrame is a Row object\nThe fields in a Row can be accessed like attributes \n\n\n#### Operations:\nTwo types of operations: transformations and actions\nTransformations are lazy (not computed immediately) Transformed DF is executed when action runs on it Persist (cache) DFs in memory or disk\n\n\nSee [Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"],"metadata":{}},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n\nprint department1\nprint employee2\nprint departmentWithEmployees1.employees[0].email"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Create dataframe from a list of Rows"],"metadata":{}},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Create a second DataFrame from a list of rows."],"metadata":{}},{"cell_type":"code","source":["departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Working with Dataframes"],"metadata":{}},{"cell_type":"code","source":["unionDF = df1.unionAll(df2)\ndisplay(unionDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Write the `Unioned` DataFrame to a Parquet file."],"metadata":{}},{"cell_type":"code","source":["# Remove the file if it exists\ndbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)\nunionDF.write.parquet(\"/tmp/databricks-df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Read from parquet file"],"metadata":{}},{"cell_type":"code","source":["parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example.parquet\")\ndisplay(parquetDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["[Explode](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#explode) the employees column"],"metadata":{}},{"cell_type":"code","source":["eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\ndisplay(eDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\neDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Read from parquet file\n\ndf = parquetDF.select(explode(\"employees\").alias(\"e\"))\nexplodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n\ndisplay(explodeDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Use `filter()` to return only the rows that match the given predicate."],"metadata":{}},{"cell_type":"code","source":["filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\n\n# Use `|` instead of `or`\nfilterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["The `where()` clause is equivalent to filter()."],"metadata":{}},{"cell_type":"code","source":["whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Replace `null` values with `--` using DataFrame Na functions."],"metadata":{}},{"cell_type":"code","source":["nonNullDF = explodeDF.fillna(\"--\")\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Retrieve only rows with missing `firstName` or `lastName`."],"metadata":{}},{"cell_type":"code","source":["filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\ndisplay(filterNonNullDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Example aggregations using `agg()` and `countDistinct()`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n  .groupBy(\"firstName\", \"lastName\")\\\n  .agg(countDistinct(\"firstName\"))\n\ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Examine the DataFrame  Physical Plans"],"metadata":{}},{"cell_type":"code","source":["countDistinctDF.explain()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["sum up all the salaries"],"metadata":{}},{"cell_type":"code","source":["salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["type(explodeDF.salary)\n"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Print the summary statistics for the salaries.\n\nexplodeDF.describe(\"salary\").show()\n"],"metadata":{},"outputs":[],"execution_count":42}],"metadata":{"name":"W3M2: Introduction to Dataframes","notebookId":1978735441444257},"nbformat":4,"nbformat_minor":0}
